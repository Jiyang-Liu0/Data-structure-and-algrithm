{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jiyang-Liu0/Data-structure-and-algrithm/blob/main/hw4_bert_pos_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune BERT-based models from Hugging Face on POS-tagging for English and Norwegian\n",
        "\n",
        "This notebook will guide you through Part 2 of [CS 2731 Homework 4](https://michaelmilleryoder.github.io/cs2731_fall2024/hw4).\n",
        "\n",
        "Please copy this notebook and name it `{pitt email id}_hw4_bert_pos.ipynb`.\n",
        "\n",
        "Code for loading and preprocessing the data is provided. You will provide code for training and evaluation using Hugging Face Trainer or PyTorch.\n",
        "\n",
        "Run all the cells starting from the top, filling in any sections that need to be filled in. Spots you need to fill in are specified.\n",
        "\n",
        "You will want to duplicate cells in each section for each language (English or Norwegian) or create separate sections in the notebook for separate languages.\n",
        "\n",
        "**Note**: Please run on GPU by going to Runtime > Change Runtime Type > T4 GPU\n",
        "\n",
        "The tutorials below from Hugging Face are informative. You can use code from them and adapt to this use case.\n",
        "* [Token classification (sequence labeling) with Hugging Face](https://huggingface.co/docs/transformers/en/tasks/token_classification)\n",
        "* [Hugging Face `Trainer` class tutorial](https://huggingface.co/docs/transformers/en/training#train)"
      ],
      "metadata": {
        "id": "v-bsGmLEA1sN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load required packages"
      ],
      "metadata": {
        "id": "zUZtkiLxA_Vv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aVyu5E0Anku"
      },
      "outputs": [],
      "source": [
        "!pip install datasets accelerate conllu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data\n",
        "\n",
        "Here you will be loading the training, dev, and test datasets of English and Norwegian text annotated with POS tags. The data are from the [Universal Dependencies](https://universaldependencies.org/) project.\n",
        "\n",
        "The dataset subset to use (fill in below for `subset_name`) are:\n",
        "* English: `en_ewt`\n",
        "* Norwegian: `no_bokmaal`\n",
        "\n",
        "We will be using the universal part-of-speech tags in the `upos` column, not the tags in the `xpos` column.\n",
        "\n",
        "Note:  There are 2 written forms of Norwegian: Bokmål and Nynorsk: https://en.wikipedia.org/wiki/Norwegian_language. This data is in the Bokmål written form.\n",
        "\n",
        "Here are a few links to learn more about the data:\n",
        "* [Universal Dependencies data format](https://universaldependencies.org/format.html)\n",
        "* [Hugging Face `universal_dependencies` dataset page](https://huggingface.co/datasets/universal_dependencies)"
      ],
      "metadata": {
        "id": "dn2M1fXE1obQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# FILL IN\n",
        "subset =  # string subset name: \"en_ewt\" for English, \"no_bokmaal\" for Norwegian\n",
        "\n",
        "data = load_dataset('universal_dependencies', subset, trust_remote_code=True)\n",
        "data"
      ],
      "metadata": {
        "id": "Q5zaRzS5A8Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the part of speech tags\n",
        "\n",
        "tags = data['train'].features['upos'].feature\n",
        "tags"
      ],
      "metadata": {
        "id": "wutZKsou-9zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a column called `upos_str` with the names, not the IDs, of POS tags\n",
        "\n",
        "def create_tag_names(batch):\n",
        "  tag_name = {'upos_str': [tags.int2str(idx) for idx in batch['upos']]}\n",
        "  return tag_name\n",
        "\n",
        "data = data.map(create_tag_names)"
      ],
      "metadata": {
        "id": "6dMaXCoNCXIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "Fill in code in this section to prepare the input with subword tokenization for BERT. You can follow the process in the [Hugging Face token classification guide](https://huggingface.co/docs/transformers/en/tasks/token_classification).\n",
        "\n",
        "Here is also where you will decide on which BERT-based pre-trained model you will fine-tune, since you will need to match its tokenization.\n",
        "Feel free to search Hugging Face for BERT variants or to use recommended ones in Hugging Face documentation. For Norwegian, you'll want a pretrained BERT model that can handle Norwegian (in Bokmål written form)."
      ],
      "metadata": {
        "id": "FM4wjjncC8P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# FILL IN with the name of a BERT-based pretrained model from Hugging Face\n",
        "pretrained_model =\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
      ],
      "metadata": {
        "id": "Hr5GJU3TC3o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subword tokenization will add special tokens such as `[CLS]` which we want the classifier to ignore.\n",
        "\n",
        "It also splits some words into multiple tokens. We'll have to re-align those to assign just one part-of-speech tag to each word.\n",
        "\n",
        "Fill in code here to do this alignment, as well as prepare a tokenized version of the dataset. You may adapt code from the [Hugging Face token classification guide](https://huggingface.co/docs/transformers/en/tasks/token_classification)."
      ],
      "metadata": {
        "id": "YAEdGRdmaf3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILL IN"
      ],
      "metadata": {
        "id": "dx7btw5lEla2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare evaluation\n",
        "\n",
        "Evaluation code is provided here.\n",
        "\n",
        "Source: [Hugging Face token classification guide](https://huggingface.co/docs/transformers/en/tasks/token_classification)"
      ],
      "metadata": {
        "id": "HrzZ7suyFiJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval\n",
        "!pip install evaluate\n",
        "\n",
        "import evaluate\n",
        "seqeval = evaluate.load('seqeval')"
      ],
      "metadata": {
        "id": "I48I5wJJFui0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "label_list = data['train'].features['upos'].feature.names\n",
        "labels = data['train'][0]['upos']\n",
        "labels = [label_list[i] for i in labels]\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "PbAcHDJbF4a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train (fine-tune) the model\n",
        "\n",
        "Fill in code here to load your pretrained model and do fine-tuning using the `Trainer` class or PyTorch."
      ],
      "metadata": {
        "id": "ey-86X_EGQCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILL IN"
      ],
      "metadata": {
        "id": "oDyIyfQtGSmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test performance\n",
        "\n",
        "Fill in code here to evaluate your fine-tuned model's performance on the test set of the tokenized dataset.\n",
        "\n",
        "You will be reporting accuracy in your report."
      ],
      "metadata": {
        "id": "D-6114reLoKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILL IN"
      ],
      "metadata": {
        "id": "EWaWR04hLerC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run on an example sentence\n",
        "\n",
        "Fill in code here to run your classifier on an example sentence of your choice for both English and Norwegian models. You will likely have to load these models from checkpoints created during training.\n",
        "\n",
        "You will provide the predicted tags for example sentences in your report."
      ],
      "metadata": {
        "id": "H2pp8gF7F410"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILL IN"
      ],
      "metadata": {
        "id": "A9L6wbiBJMhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}